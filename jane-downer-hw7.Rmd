---
title: "Homework 7"
output: html_document
html_notebook:
toc: yes
toc_float: yes
author: Jane Downer
---
  
```{r setup, include = FALSE}
library(knitr)
library(rmarkdown)
knitr::opts_chunk$set(echo=TRUE)
```

```{r}
#devtools::install_github("jabiru/tictoc")
#remove.packages("tensorflow") # May complain if not installed
#install.packages("tensorflow")
#install_tensorflow()
#tf$constant("Hello Tensorflow")
#install.packages("keras")
#install_tensorflow(method = "conda", envname = "py3.6", conda_python_version = "3.6")
# enable eager execution
# the argument device_policy is needed only when using a GPU
#library(keras)
#install_tensorflow(version = "2.3.0")
library(tensorflow)
library(keras)
library(dplyr)
library(caret)
library(stats)
library(tictoc)

rm(list=ls())

# Set working directory as needed
setwd("/Users/user/Desktop")
df <- read.csv("activity-small.csv")

# Seed the PRNG
set.seed(1122)
df <- df[sample(nrow(df)), ]

# --- Your code goes below ---
```


### Part 2.1-A
```{r}
indx <- sample(1:nrow(df), 0.20*nrow(df))
test.df  <- df[indx, ]
train.df <- df[-indx, ]

label.test <- test.df$label
test.df$label <- NULL
test.df <- as.data.frame(scale(test.df))
test.df$label <- label.test
rm(label.test)

label.train <- train.df$label
train.df$label <- NULL
train.df <- as.data.frame(scale(train.df))
train.df$label <- label.train
rm(label.train)
rm(indx)

X_train <- select(train.df, -label)
y_train <- train.df$label
y_train.ohe <- to_categorical(y_train)

X_test <- select(test.df, -label)
y_test <- test.df$label
y_test.ohe <- to_categorical(test.df$label)
```


```{r}
create_model <- function(e, batch, split, shape, HL, vary_batch = F,
                         L1_units, L1_act,
                         L2_units, L2_act,
                         L3_units, L3_act)
  {
  tic("  Time taken to train neural network")
  if (HL == 1) {
    model <- keras_model_sequential() %>%
    layer_dense(units = L1_units, activation= L1_act, input_shape=c(shape)) %>%
    layer_dense(units = L2_units, activation= L2_act)    
  } else if (HL == 2) {
    model <- keras_model_sequential() %>%
    layer_dense(units = L1_units, activation= L1_act, input_shape=c(shape)) %>%
    layer_dense(units = L2_units, activation= L2_act)  %>%  
    layer_dense(units = L3_units, activation= L3_act)
  }

  model %>% compile(loss = "categorical_crossentropy", 
                    optimizer="adam", 
                    metrics=c("accuracy"))
  
  model %>% fit(
              data.matrix(X_train),
              y_train.ohe,
              epochs = e,
              batch_size = batch, 
              validation_split = split,
              verbose = 0)
  
  acc = (model %>% evaluate(as.matrix(X_test), y_test.ohe))[2][[1]]
  acc = round(acc, 3)

  pred.class <- model %>% predict_classes(as.matrix(X_test))
  pred.prob <- model %>% predict(as.matrix(X_test)) %>% round(3)
  
  m <- confusionMatrix(as.factor(y_test), as.factor(pred.class))
  m_stats = as.data.frame(m[[4]])
  c_0 = c(round(m_stats[1,1],3), round(m_stats[1,2],3), round(m_stats[1,11],3))
  c_1 = c(round(m_stats[2,1],3), round(m_stats[2,2],3), round(m_stats[2,11],3))
  c_2 = c(round(m_stats[3,1],3), round(m_stats[3,2],3), round(m_stats[3,11],3))
  c_3 = c(round(m_stats[4,1],3), round(m_stats[4,2],3), round(m_stats[4,11],3))

  toc(log = TRUE, quiet = TRUE)
  log.txt <- tic.log(format = TRUE)
  tic.clearlog()
  
  elapsed = unlist(log.txt)
  
  line_one = ""
  if (vary_batch == F) {
    line_one = "Batch gradient descent:\n"
  } else {
    line_one = paste("Batch size: ", toString(batch), "\n",
                     "  Time taken to train neural network: ", toString(elapsed), "\n",
                     sep = "", collapse = NULL)
  }
  
  s = paste(line_one,
           "  Overall accuracy: ", toString(acc), "\n",
           "  Class 0: Sens. = ", toString(c_0[1]), ", Spec. = ",
              toString(c_0[2]), ", Bal.Acc. = ", toString(c_0[3]), "\n",
           "  Class 1: Sens. = ", toString(c_1[1]), ", Spec. = ",
              toString(c_1[2]), ", Bal.Acc. = ", toString(c_1[3]), "\n",
           "  Class 2: Sens. = ", toString(c_2[1]), ", Spec. = ",
              toString(c_2[2]), ", Bal.Acc. = ", toString(c_2[3]), "\n",
           "  Class 3: Sens. = ", toString(c_3[1]), ", Spec. = ",
              toString(c_3[2]), ", Bal.Acc. = ", toString(c_3[3]), "\n",
           sep = "", collapse = NULL)

  ret = c(s, acc)
  return(ret)
}
```


```{r}
output = create_model(e = 100, batch = 1, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = F,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)
```
### Part 2.1-A-i
```{r}
cat(paste0("Accuracy: ", output[2]))
```

### Part 2.1-A-ii
```{r}
cat(output[1])
```
### 2.1-B
```{r}
output_1   = create_model(e = 100, batch = 1, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = T,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)

output_32  = create_model(e = 100, batch = 32, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = T,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)

output_64  = create_model(e = 100, batch = 64, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = T,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)

output_128 = create_model(e = 100, batch = 128, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = T,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)

output_256 = create_model(e = 100, batch = 256, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = T,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)
```


```{r}
cat(output_1[1],"\n")
cat(output_32[1],"\n")
cat(output_64[1],"\n")
cat(output_128[1],"\n")
cat(output_256[1])
```
### Part 2.1-C-i
```{r}
cat(paste0("Increasing branch sizes results in more data entries being processed between the weights being updated. As a result, weights are updated less frequently and execution time is shorter."))
```
### Part 2.1-C-ii
```{r}
cat(paste0("With a larger batch size, the variance within batches is larger. As a result, when updating the weights, the changes vary more broadly than they do for smaller batch sizes. This leads to lower accuracy -- both balanced and overall. Note: in my results, this effect is not linear and seems to disappear around the 128-batch mark."))
```

### Part 2.1-D
```{r}
output_d_1 = create_model(e = 100, batch = 1, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = F,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)

output_d_2 = create_model(e = 100, batch = 1, split = 0.2, shape = dim(X_train)[2], HL = 2, vary_batch = F,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = dim(y_train.ohe)[2], L3_act = "softplus")

output_d_3 = create_model(e = 100, batch = 1, split = 0.2, shape = dim(X_train)[2], HL = 1, vary_batch = F,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = NULL,                L3_act = NULL)

output_d_4 = create_model(e = 100, batch = 1, split = 0.2, shape = dim(X_train)[2], HL = 2, vary_batch = F,
                      L1_units = 8,                   L1_act = "relu",
                      L2_units = dim(y_train.ohe)[2], L2_act = "softmax",
                      L3_units = dim(y_train.ohe)[2], L3_act = "softplus")
```

### Part 2.-D-i and 2.1-D-ii
```{r}
cat(output_d_1[1])
cat(output_d_2[1])
cat(output_d_3[1])
cat(output_d_4[1])
```

### Part 2.1-D
```{r}
cat(paste0("The best model had two hidden layers and batch size 1. The number of neurons per layers were 8, 4, and 4, and these layers' activation funtions were 'relu', 'softmax', and 'softplus'. The overall accuracy was 0.825, which is not all that much greater than our initial accuracy of 0.810. I noticed that changing the number of layers did not guarantee better or worse results -- in the four setups I tried in the above step, two hidden layers was associated with lower accuracy when the initial units were 16, but associated with higher accuracy when the initial units were 8." ))
```
